{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch as torch\n",
    "from pytorch_pretrained_bert import BertModel\n",
    "from pytorch_pretrained_bert import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "# from dataclasses import dataclass\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer=BertTokenizer.from_pretrained('bertPytorch/bert-base-cased')\n",
    "embedding_model=BertModel.from_pretrained('bertPytorch/bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BertEmbedding(ids,masks):\n",
    "#         input_token = ['[CLS]'] + bert_tokenizer.tokenize(text_as_input) + ['[SEP]']\n",
    "#         input_id = bert_tokenizer.convert_tokens_to_ids(input_token)\n",
    "#         input_id=[input_id]\n",
    "#         input_ids = pad_sequences(input_id, maxlen=128, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "#         attention_masks = []\n",
    "#         for seq in text_as_input_ids:\n",
    "#             for i in seq:\n",
    "#                 if i > 0:\n",
    "#                     attention_masks.append(1)\n",
    "#                 else:\n",
    "#                     attention_masks.append(0)\n",
    "#         masks=[attention_masks]\n",
    "#         masks=torch.LongTensor(masks)\n",
    "#         input_ids=torch.LongTensor(text_as_input_ids)\n",
    "        embeddings = embedding_model(ids,masks)\n",
    "        embeddings = embeddings[0]\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2dAuto(nn.Conv2d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.padding =  (self.kernel_size[0] // 2, self.kernel_size[1] // 2) # dynamic add padding based on the kernel_size\n",
    "        \n",
    "conv3x3 = partial(Conv2dAuto, kernel_size=3, bias=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = conv3x3(in_channels=32, out_channels=64)\n",
    "print(conv)\n",
    "del conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(activation):\n",
    "    return  nn.ModuleDict([\n",
    "        ['relu', nn.ReLU(inplace=True)],\n",
    "        ['leaky_relu', nn.LeakyReLU(negative_slope=0.01, inplace=True)],\n",
    "        ['selu', nn.SELU(inplace=True)],\n",
    "        ['sigmoid',nn.Sigmoid()],\n",
    "        ['none', nn.Identity()]\n",
    "    ])[activation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, activation='relu'):\n",
    "        super().__init__()\n",
    "        self.in_channels, self.out_channels, self.activation = in_channels, out_channels, activation\n",
    "        self.blocks = nn.Identity()\n",
    "        self.activate = activation_func(activation)\n",
    "        self.shortcut = nn.Identity()   \n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.should_apply_shortcut: residual = self.shortcut(x)\n",
    "        x = self.blocks(x)\n",
    "        x += residual\n",
    "        x = self.activate(x)\n",
    "        return x\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.ones((1, 1, 1, 1))\n",
    "\n",
    "block = ResidualBlock(1, 64)\n",
    "block(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetResidualBlock(ResidualBlock):\n",
    "    def __init__(self, in_channels, out_channels, expansion=1, downsampling=1, conv=conv3x3, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.expansion, self.downsampling, self.conv = expansion, downsampling, conv\n",
    "        self.shortcut = nn.Sequential(\n",
    "            nn.Conv2d(self.in_channels, self.expanded_channels, kernel_size=1,\n",
    "                      stride=self.downsampling, bias=False),\n",
    "            nn.BatchNorm2d(self.expanded_channels)) if self.should_apply_shortcut else None\n",
    "        \n",
    "        \n",
    "    @property\n",
    "    def expanded_channels(self):\n",
    "        return self.out_channels * self.expansion\n",
    "    \n",
    "    @property\n",
    "    def should_apply_shortcut(self):\n",
    "        return self.in_channels != self.expanded_channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn(in_channels, out_channels, conv, *args, **kwargs):\n",
    "    return nn.Sequential(conv(in_channels, out_channels, *args, **kwargs), nn.BatchNorm2d(out_channels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBasicBlock(ResNetResidualBlock):\n",
    "    \"\"\"\n",
    "    Basic ResNet block composed by two layers of 3x3conv/batchnorm/activation\n",
    "    \"\"\"\n",
    "    expansion = 1\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "            conv_bn(self.in_channels, self.out_channels, conv=self.conv, bias=False, stride=self.downsampling),\n",
    "            activation_func(self.activation),\n",
    "            conv_bn(self.out_channels, self.expanded_channels, conv=self.conv, bias=False),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBottleNeckBlock(ResNetResidualBlock):\n",
    "    expansion = 4\n",
    "    def __init__(self, in_channels, out_channels, *args, **kwargs):\n",
    "        super().__init__(in_channels, out_channels, expansion=4, *args, **kwargs)\n",
    "        self.blocks = nn.Sequential(\n",
    "           conv_bn(self.in_channels, self.out_channels, self.conv, kernel_size=1),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.out_channels, self.conv, kernel_size=3, stride=self.downsampling),\n",
    "             activation_func(self.activation),\n",
    "             conv_bn(self.out_channels, self.expanded_channels, self.conv, kernel_size=1),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    A ResNet layer composed by `n` blocks stacked one after the other\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, block=ResNetBasicBlock, n=1, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        # 'We perform downsampling directly by convolutional layers that have a stride of 2.'\n",
    "        downsampling = 2 if in_channels != out_channels else 1\n",
    "        self.blocks = nn.Sequential(\n",
    "            block(in_channels , out_channels, *args, **kwargs, downsampling=downsampling),\n",
    "            *[block(out_channels * block.expansion, \n",
    "                    out_channels, downsampling=1, *args, **kwargs) for _ in range(n - 1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    ResNet encoder composed by layers with increasing features.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, blocks_sizes=[64, 128, 256, 512], deepths=[2,2,2,2], \n",
    "                 activation='relu', block=ResNetBasicBlock, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.blocks_sizes = blocks_sizes\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.blocks_sizes[0], kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm2d(self.blocks_sizes[0]),\n",
    "            activation_func(activation),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.in_out_block_sizes = list(zip(blocks_sizes, blocks_sizes[1:]))\n",
    "        self.blocks = nn.ModuleList([ \n",
    "            ResNetLayer(blocks_sizes[0], blocks_sizes[0], n=deepths[0], activation=activation, \n",
    "                        block=block,*args, **kwargs),\n",
    "            *[ResNetLayer(in_channels * block.expansion, \n",
    "                          out_channels, n=n, activation=activation, \n",
    "                          block=block, *args, **kwargs) \n",
    "              for (in_channels, out_channels), n in zip(self.in_out_block_sizes, deepths[1:])]       \n",
    "        ])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.gate(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class represents the tail of ResNet. It performs a global pooling and maps the output to the\n",
    "    correct class by using a fully connected layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, n_classes):\n",
    "        super().__init__()\n",
    "        self.avg = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.decoder = nn.Linear(in_features, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.avg(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.decoder(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, n_classes, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.encoder = ResNetEncoder(in_channels, *args, **kwargs)\n",
    "        self.decoder = ResnetDecoder(self.encoder.blocks[-1].blocks[-1].expanded_channels, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet152(in_channels, n_classes, block=ResNetBottleNeckBlock, *args, **kwargs):\n",
    "    return ResNet(in_channels, n_classes, block=block, deepths=[3, 8, 36, 3], *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet152(2, 1, activation='sigmoid')\n",
    "print(model.cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeNewsClassifier(nn.Module):\n",
    "    def __init__(self, resnet_architecture, embedding_func):\n",
    "        super(FakeNewsClassifier, self).__init__()\n",
    "        self.resnet=resnet_architecture\n",
    "    def forward(self, x):\n",
    "        output = self.resnet(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FakeNewsClassifier(resnet152(128, 1, activation='sigmoid'), BertEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIds(inp):\n",
    "    ids=[]\n",
    "    masks=[]\n",
    "    for sent in inp:\n",
    "        input_token = ['[CLS]'] + bert_tokenizer.tokenize(sent) + ['[SEP]']\n",
    "        input_id = bert_tokenizer.convert_tokens_to_ids(input_token)\n",
    "        input_id=[input_id]\n",
    "        input_ids = pad_sequences(input_id, maxlen=128, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "        attention_masks = []\n",
    "        for seq in input_ids:\n",
    "            for i in seq:\n",
    "                if i > 0:\n",
    "                    attention_masks.append(1)\n",
    "                else:\n",
    "                    attention_masks.append(0)\n",
    "        maks=[attention_masks]\n",
    "        masks.append(maks)\n",
    "        ids.append(input_ids)\n",
    "    return(ids,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIds(inp):\n",
    "        ids=[]\n",
    "        masks=[]\n",
    "        for sent in inp:\n",
    "            input_token = ['[CLS]'] + bert_tokenizer.tokenize(sent) + ['[SEP]']\n",
    "            input_id = bert_tokenizer.convert_tokens_to_ids(input_token)\n",
    "            input_id=[input_id]\n",
    "            input_ids = pad_sequences(input_id, maxlen=128, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "            attention_masks = []\n",
    "            for seq in input_ids:\n",
    "                for i in seq:\n",
    "                    if i > 0:\n",
    "                        attention_masks.append(1)\n",
    "                    else:\n",
    "                        attention_masks.append(0)\n",
    "            maks=[attention_masks]\n",
    "            masks.append(maks)\n",
    "            ids.append(input_ids)\n",
    "        return(ids,masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator:\n",
    "    def __init__(self,data):\n",
    "        MAX_LEN = 128\n",
    "        batch_size=40\n",
    "        data['split'].value_counts()\n",
    "        trainDf=data[data['split']=='TRAIN']\n",
    "        valDf=data[data['split']=='VALID']\n",
    "        testDf=data[data['split']=='TEST']\n",
    "        print(trainDf.head())\n",
    "        print(valDf.head())\n",
    "        print(testDf.head())\n",
    "\n",
    "        \n",
    "        trainText = trainDf['text'].values\n",
    "        trainLabels = trainDf['truth'].values\n",
    "        valText = valDf['text'].values\n",
    "        valLabels = valDf['truth'].values\n",
    "        testText = testDf['text'].values\n",
    "        testLabels = testDf['truth'].values\n",
    "\n",
    "\n",
    "        train_ids, train_attention_masks = getIds(trainText)\n",
    "        val_ids, val_attention_masks = getIds(valText)\n",
    "        test_ids, test_attention_masks = getIds(testText)\n",
    "\n",
    "        train_data_inputs = torch.tensor(train_ids)\n",
    "        train_data_labels = torch.tensor(trainLabels)\n",
    "        train_data_masks = torch.tensor(train_attention_masks)\n",
    "        val_data_inputs = torch.tensor(val_ids)\n",
    "        val_data_labels = torch.tensor(valLabels)\n",
    "        val_data_masks = torch.tensor(val_attention_masks)\n",
    "        test_data_inputs = torch.tensor(test_ids)\n",
    "        test_data_labels = torch.tensor(testLabels)\n",
    "        test_data_masks = torch.tensor(test_attention_masks)\n",
    "\n",
    "        trainDataset = TensorDataset(train_data_inputs, train_data_masks, train_data_labels)\n",
    "        testDataset = TensorDataset(test_data_inputs, test_data_masks, test_data_labels)\n",
    "        valDataset = TensorDataset(val_data_inputs, val_data_masks, val_data_labels)\n",
    "        train_data_sampler=RandomSampler(trainDataset)\n",
    "        test_data_sampler=RandomSampler(testDataset)\n",
    "        val_data_sampler=RandomSampler(valDataset)\n",
    "\n",
    "        self.train_dataloader = DataLoader(trainDataset, sampler=train_data_sampler, batch_size=batch_size)\n",
    "        self.valid_dataloader = DataLoader(valDataset, sampler=test_data_sampler, batch_size=batch_size)\n",
    "        self.test_dataloader = DataLoader(testDataset,sampler=test_data_sampler, batch_size=batch_size)\n",
    "\n",
    "        \n",
    "    def generate_train_batch_data(self):\n",
    "        for i, batch in enumerate(self.train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            yield batch\n",
    "\n",
    "    def generate_valid_batch_data(self):\n",
    "        for i, batch in enumerate(self.valid_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            yield batch\n",
    "\n",
    "    def generate_test_batch_data(self):\n",
    "        for i, batch in enumerate(self.test_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGeneratorv2:\n",
    "    def __init__(self):\n",
    "        MAX_LEN = 128\n",
    "        batch_size=40\n",
    "        data=pd.read_csv('/home/gridsan/svattam/upama/FakeNews/combined_relevant_data.csv')\n",
    "        df = data.sample(frac=1).reset_index(drop=True)\n",
    "        data=df.iloc[0:1000]\n",
    "        text=data[['text']]\n",
    "        labels=data[['truth']].to_numpy()\n",
    "        tokenizer= BertTokenizer.from_pretrained('/home/gridsan/svattam/bert/bert-base-cased', do_lower_case=True)\n",
    "        x_train, X_test, Y_train, y_test = train_test_split(text,labels, test_size=0.3)\n",
    "        x_val, x_test, Y_val, Y_test = train_test_split(X_test, y_test, test_size=0.5)\n",
    "        \n",
    "        train_inputs, train_attention_masks = prepare_for_bert(x_train, tokenizer, MAX_LEN)\n",
    "        val_inputs, val_attention_masks = prepare_for_bert(x_val, tokenizer, MAX_LEN)\n",
    "        test_inputs, test_attention_masks = prepare_for_bert(x_test, tokenizer, MAX_LEN)\n",
    "        \n",
    "        train_data_inputs = torch.tensor(train_inputs)\n",
    "        train_data_labels = torch.tensor(Y_train)\n",
    "        train_data_masks = torch.tensor(train_attention_masks)\n",
    "        val_data_inputs = torch.tensor(val_inputs)\n",
    "        val_data_labels = torch.tensor(Y_val)\n",
    "        val_data_masks = torch.tensor(val_attention_masks)\n",
    "        test_data_inputs = torch.tensor(test_inputs)\n",
    "        test_data_labels = torch.tensor(Y_test)\n",
    "        test_data_masks = torch.tensor(test_attention_masks)\n",
    "       \n",
    "        trainDataset = TensorDataset(train_data_inputs, train_data_masks, train_data_labels)\n",
    "        testDataset = TensorDataset(test_data_inputs, test_data_masks, test_data_labels)\n",
    "        valDataset = TensorDataset(val_data_inputs, val_data_masks, val_data_labels)\n",
    "        train_data_sampler=RandomSampler(trainDataset)\n",
    "        test_data_sampler=RandomSampler(testDataset)\n",
    "        val_data_sampler=RandomSampler(valDataset)\n",
    "\n",
    "\n",
    "        # self.train_data = \n",
    "        # self.train_masks = \n",
    "        # self.train_labels = \n",
    "\n",
    "        # self.valid_data = \n",
    "        # self.valid_masks = \n",
    "        # self.valid_labels = \n",
    "\n",
    "        self.train_dataloader = DataLoader(trainDataset, sampler=train_data_sampler, batch_size=batch_size)\n",
    "        self.valid_dataloader = DataLoader(valDataset, sampler=test_data_sampler, batch_size=batch_size)\n",
    "        self.test_dataloader = DataLoader(testDataset,sampler=test_data_sampler, batch_size=batch_size)\n",
    "\n",
    "    # def sampleFromClass(ds, k):\n",
    "    #     class_counts = {}\n",
    "    #     train_data = []\n",
    "    #     train_label = []\n",
    "    #     test_data = []\n",
    "    #     test_label = []\n",
    "    #     for data, label in ds:\n",
    "    #         c = label.item()\n",
    "    #         class_counts[c] = class_counts.get(c, 0) + 1\n",
    "    #         if class_counts[c] <= k:\n",
    "    #             train_data.append(data)\n",
    "    #             train_label.append(torch.unsqueeze(label, 0))\n",
    "    #         else:\n",
    "    #             test_data.append(data)\n",
    "    #             test_label.append(torch.unsqueeze(label, 0))\n",
    "    #     train_data = torch.cat(train_data)\n",
    "    #     for ll in train_label:\n",
    "    #         print(ll)\n",
    "    #     train_label = torch.cat(train_label)\n",
    "    #     test_data = torch.cat(test_data)\n",
    "    #     test_label = torch.cat(test_label)\n",
    "\n",
    "    #     return (TensorDataset(train_data, train_label), \n",
    "    #         TensorDataset(test_data, test_label))\n",
    "        \n",
    "    def generate_train_batch_data(self):\n",
    "        for i, batch in enumerate(self.train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            yield batch\n",
    "\n",
    "    def generate_valid_batch_data(self):\n",
    "        for i, batch in enumerate(self.valid_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            yield batch\n",
    "\n",
    "    def generate_test_batch_data(self):\n",
    "        for i, batch in enumerate(self.test_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            yield batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('/home/gridsan/svattam/upama/FakeNews/combined_relevant_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.sample(frac=1).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data=df.iloc[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_bert(df, tokenizer, MAX_LEN):\n",
    "    sentences=df['text'].values\n",
    "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n",
    "    input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    attention_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen=DataGeneratorv2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = BertModel.from_pretrained('/home/gridsan/svattam/bert/bert-base-cased', from_tf=False)\n",
    "        self.embedding_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 40\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss()\n",
    "metrics=[]\n",
    "for epoch in range(n_epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = []\n",
    "    val_loss=[]\n",
    "    train_acc=[]\n",
    "    val_acc=[]\n",
    "    for i, batch in enumerate(data_gen.generate_train_batch_data()):\n",
    "        print(i)\n",
    "        model.train(True)\n",
    "        b_input_ids, b_input_mask, labels = batch\n",
    "        print(b_input_ids.size(),b_input_mask.size())\n",
    "        with torch.no_grad():\n",
    "            embeddings = embedder.embedding_model(b_input_ids, attention_mask=b_input_mask)\n",
    "            embeddings=torch.stack(embeddings[0], dim=3)\n",
    "        X_batch = embeddings.float()\n",
    "#         X_batch = X_batch.unsqueeze(3)\n",
    "        y_batch = labels.float()\n",
    "        y_pred = model(X_batch)\n",
    "        print(\"=================\")\n",
    "        print(y_batch)\n",
    "        print(y_pred)\n",
    "        print(\"=================\")\n",
    "        exit()\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_labels=y_pred.cpu().detach().numpy()\n",
    "        cat=np.argmax(y_labels, axis=1)\n",
    "        clabels=np.argmax(labels.cpu().detach().numpy(), axis=1)\n",
    "        train_acc.append(accuracy_score(clabels,cat))\n",
    "    for i, batch in enumerate(data_gen.generate_valid_batch_data()):\n",
    "        print(i)\n",
    "        correct=0\n",
    "        model.eval()\n",
    "        b_input_ids, b_input_mask, labels = batch\n",
    "        with torch.no_grad():\n",
    "            embeddings = embedder.embedding_model(b_input_ids, attention_mask=b_input_mask)\n",
    "            embeddings=torch.stack(embeddings[0], dim=3)\n",
    "        X_batch = embeddings.float()\n",
    "#         X_batch = embeddings[0].float()\n",
    "        y_batch = labels.float()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        val_loss.append(loss.item())\n",
    "        y_labels=y_pred.cpu().detach().numpy()\n",
    "        cat=np.argmax(y_labels, axis=1)\n",
    "        clabels=np.argmax(labels.cpu().detach().numpy(), axis=1)\n",
    "        val_acc.append(accuracy_score(clabels,cat))   \n",
    "    trainingAccuracy=statistics.mean(train_acc)\n",
    "    validationAccuracy=statistics.mean(val_acc)\n",
    "    trainingLoss=statistics.mean(train_loss)\n",
    "    validationLoss=statistics.mean(val_loss)\n",
    "    metrics.append([trainingAccuracy,trainingLoss,validationAccuracy,validationLoss])\n",
    "print(metrics)\n",
    "stats = np.array(metrics)\n",
    "np.savez(\"trainingstats.npz\", stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-transformerGAN]",
   "language": "python",
   "name": "conda-env-.conda-transformerGAN-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
