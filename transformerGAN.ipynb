{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerGAN\n",
    "\n",
    "An implementation of text GAN which uses BERT sequence classifier as the discriminator and OpenAI's GPT-2 as the generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, datetime\n",
    "import sys\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "from gpt2Pytorch.GPT2.model import GPT2LMHeadModel\n",
    "from gpt2Pytorch.GPT2.utils import load_weight\n",
    "from gpt2Pytorch.GPT2.config import GPT2Config\n",
    "from gpt2Pytorch.GPT2.sample import sample_sequence\n",
    "from gpt2Pytorch.GPT2.encoder import get_encoder\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer\n",
    "from torch import nn\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam, OpenAIAdam\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from torch.nn import functional as F\n",
    "from pandas import Series, DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__ # should be 1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrtansformerGAN(object):\n",
    "    def __init__(self, dataframe, bert_tokenizer, bert_classifier, gpt_tokenizer, gpt_generator, num_labels):\n",
    "        self.dataframe = dataframe\n",
    "        self.device_default = device\n",
    "        \n",
    "        # Build discriminator and tokenizer from BertForSequenceClassification\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        self.discriminator = nn.DataParallel(bert_classifier).to(self.device_default)\n",
    "        self.bert_optimizer = BertAdam(self.discriminator.parameters(), lr = 0.00005, warmup = 0.1, t_total = 1000)\n",
    "        \n",
    "        # Build the generator, tokenizer, optimizer from OpenAIGPT2\n",
    "        self.gpt2_tokenizer = gpt_tokenizer\n",
    "        self.generator = gpt_generator.to(self.device_default)\n",
    "        self.gpt2_optimizer = OpenAIAdam(self.generator.parameters(), lr = 0.0001, warmup = 0.1, t_total = 1000)\n",
    "        \n",
    "        # Free all GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    def textGeneration(self, generator_input):\n",
    "        text_id = generator_input\n",
    "        input, past = torch.tensor([text_id]).to(self.device_default), None\n",
    "        for _ in range(random.randint(30, 100)):\n",
    "            logits, past = self.generator(input, past = past)\n",
    "            input = torch.multinomial(F.softmax(logits[:, -1]), 1)\n",
    "            text_id.append(input.item())\n",
    "        return self.gpt2_tokenizer.decode(text_id)\n",
    "    \n",
    "    def dataGenerator(self, batch_size = 16):\n",
    "        # Randomly fetch traning data bunch\n",
    "        sample_text_ss = self.dataframe['text'].iloc[random.sample(range(len(self.dataframe)), batch_size)]\n",
    "        \n",
    "        # Tokenize training data bunch with GPT2 tokenizer and take top 10 words\n",
    "        sample_text_encode_top10 = sample_text_ss.map(lambda x : self.gpt2_tokenizer.encode(x)[:10])\n",
    "        \n",
    "        # Generate text using GPT2 generator\n",
    "        sample_text_generate_ss = sample_text_encode_top10.map(self.textGeneration)\n",
    "        return sample_text_generate_ss, sample_text_ss\n",
    "    \n",
    "    def discriminatorInput(self, text):\n",
    "        input_token = ['[CLS]'] + self.bert_tokenizer.tokenize(text) + ['[SEP]']\n",
    "        input_id = self.bert_tokenizer.convert_tokens_to_ids(input_token)\n",
    "        return [input_id]\n",
    "    \n",
    "    def saveGeneratedText(self):\n",
    "        content = self.dataframe['text'].values[random.randint(0, len(self.dataframe))]\n",
    "        content_id = self.gpt2_tokenizer.encode(content)[:10]\n",
    "        gen_content = self.textGeneration(content_id)\n",
    "        gen_content.strip()\n",
    "        return gen_content, content\n",
    "        \n",
    "    def train(self, num_epochs = 10, save_interval = 2):\n",
    "        start = datetime.now()\n",
    "        generated_text_list = []\n",
    "        real_text_list = []\n",
    "        d_loss_list = []\n",
    "        g_loss_list = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            try:\n",
    "                print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "                print('-' * 10)\n",
    "\n",
    "                # Load in data\n",
    "                sample_text_generate_ss, sample_text_ss = self.dataGenerator(batch_size = 16)\n",
    "\n",
    "                # Convert generated text and real text bunch to WorkPiece encode ID as discriminator input\n",
    "                discriminator_input_ss = pd.concat([sample_text_generate_ss, sample_text_ss], axis = 0, ignore_index = True).map(self.discriminatorInput)\n",
    "                discriminator_input = torch.LongTensor(np.array(DataFrame(discriminator_input_ss.sum()).fillna(0).astype('int32'))).to(self.device_default)\n",
    "                discriminator_input_generate = discriminator_input[:len(sample_text_generate_ss)].to(self.device_default)\n",
    "\n",
    "                # Create labels for training discriminator and generator\n",
    "                labels = torch.LongTensor([0] * len(sample_text_generate_ss) + [1] * len(sample_text_ss)).to(self.device_default)\n",
    "                valid = torch.LongTensor([1] * len(sample_text_ss)).to(self.device_default)\n",
    "\n",
    "                # Each epoch has a train_discriminator and train_generator phase\n",
    "                for phase in ['train_discriminator', 'train_generator']:\n",
    "                    if phase == 'train_discriminator':\n",
    "                        # Set discriminator to training mode\n",
    "                        self.discriminator.train()\n",
    "\n",
    "                        # Freeze all trainable parameters\n",
    "                        for param in self.discriminator.parameters():\n",
    "                            param.requires_grad = True\n",
    "\n",
    "                        # Zero the discriminator parameter gradients\n",
    "                        self.bert_optimizer.zero_grad()\n",
    "\n",
    "                        # Forward propagation\n",
    "                        d_loss = self.discriminator(input_ids = discriminator_input, labels = labels).mean()\n",
    "\n",
    "                        # Backward propagation\n",
    "                        d_loss.backward()\n",
    "                        self.bert_optimizer.step()\n",
    "\n",
    "                    else:\n",
    "                        # Set discriminator to evaluate mode\n",
    "                        self.discriminator.eval()\n",
    "\n",
    "                        # Zero the generator parameter gradients\n",
    "                        self.gpt2_optimizer.zero_grad()\n",
    "\n",
    "                        # Forward propagation\n",
    "                        g_loss = self.discriminator(input_ids = discriminator_input_generate, labels = valid).mean()\n",
    "\n",
    "                        # Backward propagation\n",
    "                        g_loss.backward()\n",
    "                        self.gpt2_optimizer.step()                    \n",
    "\n",
    "                # Plot the progress\n",
    "                print('Discriminator Loss:', d_loss)\n",
    "                print('Generator Loss:', g_loss)\n",
    "                print()\n",
    "                d_loss_list.append(d_loss)\n",
    "                g_loss_list.append(g_loss)\n",
    "\n",
    "                # If at save interval, then save generated text samples\n",
    "                if epoch % save_interval == 0:\n",
    "                    generated_text, real_text = self.saveGeneratedText()\n",
    "                    \n",
    "                    file_object = open('gen_textlog.txt', 'a')\n",
    "                    file_object.write(generated_text)\n",
    "                    file_object.write('\\n----------------------------------------------------------\\n')\n",
    "                    file_object.close()\n",
    "                    \n",
    "                    # generated_text_list.append(generated_text)\n",
    "                    # real_text_list.append(real_text)\n",
    "            except RuntimeError:\n",
    "                pass\n",
    "\n",
    "        # Counting time elapsed\n",
    "        time_delta = datetime.now() - start\n",
    "        print('Training completed time:', time_delta)\n",
    "\n",
    "        return self.generator, self.discriminator, d_loss_list, g_loss_list, generated_text_list, real_text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pretrained_model_path = 'bertPytorch/bert-base-cased'\n",
    "gpt2_model_path = 'gpt2Pytorch/gpt2-pytorch_model.bin'\n",
    "gpt2_vocab_path = 'gpt2Pytorch/GPT2-vocab'\n",
    "assert os.path.exists(bert_pretrained_model_path)\n",
    "assert os.path.exists(gpt2_model_path)\n",
    "assert os.path.exists(gpt2_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_pretrained_model_path)\n",
    "bert_for_seq_classification = BertForSequenceClassification.from_pretrained(bert_pretrained_model_path, num_labels = num_labels)\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(gpt2_vocab_path)\n",
    "state_dict = torch.load(gpt2_model_path, map_location='cpu' if not torch.cuda.is_available() else None)\n",
    "enc = get_encoder()\n",
    "config = GPT2Config()\n",
    "gpt2_model = GPT2LMHeadModel(config)\n",
    "gpt2_model = load_weight(gpt2_model, state_dict)\n",
    "gpt2_model.to(device)\n",
    "_ = gpt2_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33252, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sampledata.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "textgan = TrtansformerGAN(\n",
    "    dataframe = df, \n",
    "    bert_tokenizer = bert_tokenizer, \n",
    "    bert_classifier = bert_for_seq_classification, \n",
    "    gpt_tokenizer = gpt2_tokenizer, \n",
    "    gpt_generator = gpt2_model, \n",
    "    num_labels = num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/SW26425/.conda/envs/fibber/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/gridsan/SW26425/.conda/envs/fibber/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: tensor(0.6271, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(0.7504, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.6393, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(0.7331, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.6262, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(0.7091, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.5665, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(0.7194, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.5436, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(0.7588, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.4935, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(1.0125, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.4345, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(1.0798, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.4657, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(1.1380, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.4220, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(1.1584, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "Discriminator Loss: tensor(0.3509, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "Generator Loss: tensor(1.2278, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Training completed time: 0:02:42.197474\n"
     ]
    }
   ],
   "source": [
    "OpenAIGPT2_generator, BERT_discriminator, d_loss_list, g_loss_list, generated_review_list, real_review_list = textgan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fibber]",
   "language": "python",
   "name": "conda-env-.conda-fibber-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
